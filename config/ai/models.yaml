# LAIA AI Model Configuration
# 
# LAIA supports three AI modes:
#   - online:  Use a free cloud API (recommended — no download, no GPU)
#   - local:   Run Ollama on this machine
#   - lan:     Connect to Ollama on your LAN

# ─── ONLINE FREE MODELS ────────────────────────────────────────────────────
# See config/ai/providers.yaml for full provider details and API setup
online:
  default_provider: groq
  default_model: llama-3.1-8b-instant
  recommended:
    - provider: groq
      model: llama-3.1-8b-instant
      reason: "Fastest free inference, no setup beyond API key"
    - provider: openrouter
      model: "meta-llama/llama-3.1-8b-instruct:free"
      reason: "Access to many free models through one key"
    - provider: google
      model: "gemini-2.0-flash"
      reason: "Vision-capable, multilingual, generous free tier"

# ─── LOCAL MODELS (Ollama) ──────────────────────────────────────────────────
local:
  default_model: gemma3:4b

  tiers:
    minimal:
      install_default: ["gemma3:1b", "gemma3:4b"]
      optional: ["phi4-mini", "llama3.2:3b"]

    standard:
      install_default: ["gemma3:4b"]
      optional: ["deepseek-r1:7b", "qwen2.5-coder:7b", "llama3.3:70b"]

    powerful:
      install_default: ["gemma3:12b"]
      optional: ["qwen3:8b", "deepseek-r1:70b"]

  catalog:
    - id: "gemma3:1b"
      name: "Gemma 3 (1B)"
      description: "Tiny but capable. Vision-enabled. 1GB download."
      ram_gb: 1
      use_case: "general"

    - id: "gemma3:4b"
      name: "Gemma 3 (4B)"
      description: "Best compact local model (Feb 2026). Vision, reasoning, fast."
      ram_gb: 4
      use_case: "general"
      recommended: true

    - id: "llama3.2:3b"
      name: "Llama 3.2 (3B)"
      description: "Meta's compact model. Tool-calling capable."
      ram_gb: 3
      use_case: "general"

    - id: "phi4-mini"
      name: "Phi-4 Mini"
      description: "Microsoft's compact reasoning model. Excellent for coding."
      ram_gb: 4
      use_case: "coding"

    - id: "deepseek-r1:7b"
      name: "DeepSeek R1 (7B)"
      description: "Best open reasoning model. Think step-by-step."
      ram_gb: 7
      use_case: "reasoning"

    - id: "qwen2.5-coder:7b"
      name: "Qwen 2.5 Coder (7B)"
      description: "Best coding model at 7B. Code completion + explanation."
      ram_gb: 7
      use_case: "coding"

    - id: "gemma3:12b"
      name: "Gemma 3 (12B)"
      description: "Powerful Google model with vision. Needs 16GB RAM."
      ram_gb: 12
      use_case: "general"
      warning: "Requires 16GB RAM"

    - id: "llama3.3:70b"
      name: "Llama 3.3 (70B)"
      description: "Near-frontier open model quality."
      ram_gb: 40
      use_case: "general"
      warning: "Requires 40GB+ RAM or GPU"

    - id: "deepseek-r1:70b"
      name: "DeepSeek R1 (70B)"
      description: "Best open reasoning model. Approaches o3 quality."
      ram_gb: 40
      use_case: "reasoning"
      warning: "Requires 40GB+ RAM or GPU"

# ─── LAN REMOTE ─────────────────────────────────────────────────────────────
lan:
  description: "Connect to an Ollama instance running on your local network."
  setup: "Run 'laia-setup' and choose option 7 (LAN Remote)"
  server_setup_hint: |
    On the server machine, set Ollama to listen on all interfaces:
    echo 'OLLAMA_HOST=0.0.0.0' | sudo tee -a /etc/systemd/system/ollama.service
    sudo systemctl daemon-reload && sudo systemctl restart ollama
